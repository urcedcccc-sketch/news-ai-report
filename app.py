import streamlit as st
import requests
from openai import OpenAI
from datetime import datetime, timedelta
from newspaper import Article  # æ–°å¢ï¼šç”¨äºæŠ“å–ç½‘é¡µæ­£æ–‡

# 1. å¯†é’¥åˆå§‹åŒ–
try:
    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
    TIAN_API_KEY = st.secrets["TIAN_API_KEY"]
except Exception as e:
    st.error("å¯†é’¥é…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥ Streamlit Secretsã€‚")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# é¡µé¢è®¾ç½®
st.set_page_config(page_title="å…¨åŸŸæ™ºèƒ½å†…å‚ç³»ç»Ÿ", layout="wide")
st.title("ğŸ—ï¸ å…¨åŸŸæ™ºèƒ½å†…å‚ç³»ç»Ÿ")
st.caption("å¤šæºå¹¶å‘æ£€ç´¢ | å®æ—¶æ­£æ–‡æŠ“å– | AI æ·±åº¦æç‚¼")

# 2. ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("æ£€ç´¢è®¾ç½®")
    word = st.text_input("è¯·è¾“å…¥æ ¸å¿ƒå…³é”®è¯", "ä¼Šæœ—")
    num_limit = st.slider("æœ€å¤§å±•ç¤ºæ€»ç¯‡æ•°", 1, 30, 8)
    st.divider()
    btn = st.button("å¼€å§‹åŒæ­¥å…¨åŸŸå†…å‚", type="primary")

# 3. æ ¸å¿ƒå·¥å…·å‡½æ•°
def get_full_text(url):
    """æ–°å¢ï¼šç»™å®šURLï¼ŒæŠ“å–å¹¶è¿”å›ç½‘é¡µæ­£æ–‡"""
    try:
        article = Article(url, language='zh')
        article.download()
        article.parse()
        return article.text
    except:
        return None

def is_within_a_week(date_str):
    if not date_str: return False
    try:
        fmt = "%Y-%m-%d %H:%M:%S" if ":" in date_str else "%Y-%m-%d"
        news_date = datetime.strptime(date_str[:19], fmt)
        return datetime.now() - news_date <= timedelta(days=7)
    except: return True

def fetch_all_sources(kw):
    endpoints = {
        "å›½é™…æ–°é—»": "https://apis.tianapi.com/world/index",
        "å›½å†…æ–°é—»": "https://apis.tianapi.com/guonei/index",
        "äº’è”ç½‘èµ„è®¯": "https://apis.tianapi.com/internet/index",
        "ç»¼åˆæ–°é—»": "https://apis.tianapi.com/generalnews/index"
    }
    aggregated_news = []
    for name, url in endpoints.items():
        params = {"key": TIAN_API_KEY, "num": 30, "word": kw.strip()}
        try:
            res = requests.get(url, params=params, timeout=8).json()
            if res.get("code") == 200:
                news_list = res.get("result", {}).get("newslist", [])
                for n in news_list:
                    n["source_tag"] = name 
                aggregated_news.extend(news_list)
        except: continue
    aggregated_news.sort(key=lambda x: x.get('ctime', ''), reverse=True)
    return aggregated_news

# 4. ä¸»æ¸²æŸ“é€»è¾‘
if btn:
    if not word:
        st.warning("âš ï¸ è¯·è¾“å…¥å…³é”®è¯")
        st.stop()

    status = st.empty()
    status.info(f"æ­£åœ¨è·¨æºåŒæ­¥ã€{word}ã€å¹¶å®æ—¶è§£ææ­£æ–‡...")
    
    all_data = fetch_all_sources(word)
    final_list = [n for n in all_data if is_within_a_week(n.get('ctime'))][:num_limit]

    if not final_list:
        st.error(f"ğŸ” æœªå‘ç°æœ€è¿‘ 7 å¤©å†…å…³äºã€{word}ã€çš„æœ‰æ•ˆæŠ¥é“ã€‚")
    else:
        for news in final_list:
            with st.container(border=True):
                title = news.get('title', 'æ— æ ‡é¢˜')
                source = news.get('source', 'æƒå¨åª’ä½“')
                url = news.get('url')
                
                col1, col2 = st.columns([1, 4])
                with col1:
                    st.write(f"**{source}**")
                    st.caption(f"ğŸ“… {news.get('ctime')}")
                    st.caption(f"ğŸ“‚ {news.get('source_tag')}")
                
                with col2:
                    st.markdown(f"### {title}")
                    
                    # ç¬¬ä¸€æ­¥ï¼šå°è¯•æŠ“å–ç½‘é¡µçœŸå®æ­£æ–‡
                    full_content = None
                    if url:
                        with st.spinner('æ­£åœ¨æ·±åº¦è§£æåŸæ–‡...'):
                            full_content = get_full_text(url)
                    
                    # ç¬¬äºŒæ­¥ï¼šç¡®å®šäº¤ç»™ AI çš„ç´ æï¼ˆæŠ“å–çš„æ­£æ–‡ > APIæ‘˜è¦ > æ ‡é¢˜ï¼‰
                    content_for_ai = full_content or news.get('description') or news.get('digest')
                    
                    if content_for_ai and len(content_for_ai) > 20:
                        try:
                            prompt = (
                                f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ”¿ç»åˆ†æå¸ˆã€‚è¯·é’ˆå¯¹ä»¥ä¸‹æ–°é—»ç´ æè¿›è¡Œæ·±åº¦æç‚¼ï¼š\n"
                                f"ã€æ ‡é¢˜ã€‘ï¼š{title}\n"
                                f"ã€ç´ æå†…å®¹ã€‘ï¼š{content_for_ai[:1500]}\n\n" # æˆªå–å‰1500å­—é˜²æ­¢è¶…é•¿
                                f"è¦æ±‚ï¼šè¯·å†™ä¸€æ®µ150å­—ä»¥å†…çš„æ·±åº¦æ€»ç»“ã€‚è¦æ±‚ï¼š\n"
                                f"1. æ¦‚æ‹¬äº‹ä»¶æ ¸å¿ƒäº‹å®ï¼›\n"
                                f"2. åˆ†æè¯¥äº‹ä»¶èƒŒåçš„æ½œåœ¨å½±å“æˆ–é‡è¦æ€§ï¼›\n"
                                f"3. è¯­æ°”ä¿æŒä¸¥è°¨ã€å®¢è§‚ã€ä¸“ä¸šã€‚ä¸è¦å‡ºç°'æ ¹æ®ç´ æ'ç­‰åºŸè¯ã€‚"
                            )
                            
                            completion = client.chat.completions.create(
                                model="gpt-4o-mini",
                                messages=[{"role": "user", "content": prompt}],
                                temperature=0.3
                            )
                            st.info(completion.choices[0].message.content)
                        except:
                            st.write(content_for_ai[:200] + "...")
                    else:
                        st.warning("ğŸš¨ æ— æ³•æå–æœ‰æ•ˆæ­£æ–‡ï¼ŒAI æ€»ç»“è·³è¿‡ã€‚")
                    
                    if url:
                        st.markdown(f"ğŸ”— [é˜…è¯»åŸå‘æŠ¥é“]({url})")
    status.empty()
